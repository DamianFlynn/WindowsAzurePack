==== Networking
The objective of this phase of the configuration, is to implement a network configuration model in VMM, which will then be deployed as a real configuration on our hosts, and ultimately enabling the placement of our virtual machines, and host virtual networks.

IMPORTANT: Focus Server is **PDC-SC-VMM1**

Starting this part of the configuration process, we should have for the initial hosts, already placed them under VMM Management, so that we can walk through the configuration procedure, and experience the results of the steps we are going to excerpt on the system.

Illustrated, we can see the single host which we placed under VMM management a little earlier. In the view, we are looking at the *Hosts* pivot of the *Networking* node, on the *Fabric* view.

.SCVMM Network - Host Network View
image::images/SCVMM2012R2-NETWORK-0000.png[PKI Post Deployment Configuration]

In the main pane, we can see that the host, has the following configuration currently implemented

* 2 Broadcom NetXtreme 1Gb Ethernet Cards
* The 2 physical network interfaces (pNIC's) have being combined to a single team
* A Hyper-V Switch connected to the team (Distributed Switch)

This represents a very common initial configuration scenario, where the hosts have previously being prepared for Hyper-V, and simply added to VMM for Management. In this configuration we have no *Logical Switch*, and will have to fully remove both the *Distributed Switch* and *Network Team* in order to fully reconfigure the host, for true VMM Management.

As one of the more complex starting points, we will use this scenario as we progress our implementation of the logical switch.

===== Network Plan
There is no hope of covering all the details involved in the planning of a Network Design for VMM in this Build Guide, but to be honest this would not be the appropriate place to host this information either. Instead I will direct you to read the Free eBook [Microsoft System Center: Building a Virtualized Network Solution](http://blogs.msdn.com/b/microsoft_press/archive/2014/02/18/free-ebook-microsoft-system-center-building-a-virtualized-network-solution.aspx), in which we cover this topic in extensive depth.

In order to guide us through the creation of a Network Stack, We will start with a *Network Plan* which contains a lot of the common elements we see as we deploy converged networks.

===== Network Elements
VMM enables a lot of flexibility while designing and implementing networks, installation started with a concept we initially call a *Model*, which ultimately will get deployed as a *Logical Switch* when everything is combined together.

To establish this *Model* of your physical environment, we have a number of different elements of the physical network which we can define, each of these describing a dedicated sections. Each of these elements relate to each other, the following graphic illustrates this affinity.

[graphviz, scvmm-network-elements, svg]
----
digraph g {
    Z1 -> Z2 -> Z3 -> Z5
    Z2 -> Z4 -> Z6 -> Z7 -> Z8
    Z9 -> Z7

    Z1 [label="Logical Switch",style=filled]
    Z2 [label="Port Profiles"]
    Z3 [label="Uplink Port Profiles",style=filled]
    Z4 [label="Virtual Port Profiles",style=filled]
    Z5 [label="pNIC's and Team"]
    Z6 [label="Logical Networks",style=filled]
    Z7 [label="Logical Network Sites",style=filled]
    Z8 [label="IP Pools",style=filled]
    Z9 [label="Virtual Networks",style=filled]
}
----

The Items in the above graph which are prefixed with the ** * ** are elements within VMM which we will be defining in the course of this process.

===== Fully Converged Network
Our next step is to visualize what a fully converged network might actually look like in a model. To help explain this concept, the following diagram leverages a single Logical Switch as the root of the configuration.

This Logical Switch hosts the Physical Uplink Port Profiles which are responsible to the configuration details of the physical interfaces on the hosts which will be connected to the Logical Switch; including whether or not these are to be teamed, and indirectly to indicate what VLANs will be passed from the Virtual Interfaces trough these physical adapters.

Therefore one of the more important points that is never directly called out, but instead always inferred is that there is an automatic assumption that the Physical Switch to which these Physical Ports are connected to, has the correct, or rather the appropriate configuration in place to accept the VLAN tags which are being presented and therefore passed over the layer 2 network.

The actual configuration of the physical switch is really beyond the scope of what we will cover here, as every switch has their own unique method of configuration, and actually naming norm enculture also; for example on one device we might be looking at a VLAN, another this might be just called 802.11q

.Defining VLAN's on Networking Hardware
****
For reference purposes, the following is a sample of the User eXperience in the Mikrotik RouterOS platform, exposing the VLANS and IP addresses for each vlan which has being defined.

.Network Routers - Sample VLAN creation based on Mikrotik
image::images/SCVMM2012R2-NETWORK-0006.png[PKI Post Deployment Configuration]

And, this is the same information, as exposed trough the shell interface on the Mikrotik device

```
/interface bonding
add link-monitoring=none name="LACP Bond" slaves=eth2,eth3,eth5,eth4
/interface vlan
add interface="LACP Bond" name="vlan110 [PDC LAN]" vlan-id=110
add interface="LACP Bond" name="vlan120 [PDC Storage]" vlan-id=120
add interface="LACP Bond" name="vlan150 [PDC Management]" vlan-id=150
add interface="LACP Bond" name="vlan160 [PDC Cluster]" vlan-id=160
add interface="LACP Bond" name="vlan161 [PDC Live Migration]" vlan-id=161
add interface="LACP Bond" name="vlan162 [PDC Provider]" vlan-id=162
add interface="LACP Bond" name="vlan180 [PDC Administration]" vlan-id=180
add interface="LACP Bond" name="vlan190 [PDC DMZ]" vlan-id=190
/ip address
add address=172.21.10.1/24 comment="PDC LAN" interface="vlan110 [PDC LAN]" \
   network=172.21.10.0
add address=172.21.150.1/24 comment=":: PDC Management" interface=\
   "vlan150 [PDC Management]" network=172.21.150.0
add address=172.21.80.1/24 comment="PDC Secure DMZ" interface=\
   "vlan180 [PDC Administration]" network=172.21.80.0
add address=172.21.90.1/24 comment="PDC Unsecure DMZ" interface=\
   "vlan190 [PDC DMZ]" network=172.21.90.0
add address=172.21.60.0/24 comment="PDC Cluster" interface=\
   "vlan160 [PDC Cluster]" network=172.21.60.0
add address=172.21.61.0/24 comment="PDC Live Migration" interface=\
   "vlan161 [PDC Live Migration]" network=172.21.61.0
add address=172.21.62.0/24 comment="PDC Provider / Control Plane" interface=\
   "vlan162 [PDC Provider]" network=172.21.62.0
```
****

Regardless of the hardware platform which is leveraged for the environment, the tagging requirements will remain the same, just the method of implementation will differ.

The following diagram represents a typical design of a Full Converged Network, implemented with a single Logical Switch at the heart of the configuration. This design uses an Uplink Port Profile to define the teaming of the physical Networks, and a set of Virtual Port Profiles to defined the traffic which will be passed through this switch.

A set of 3 difference Logical Networks is defined to manage the different logical segmentation of the connected workloads, including

* Management Networks - *Includes VLANs, for each of Management, Clustering and Live Migration network workloads*
* Tenant Networks - *Includes VLANs which are each dedicated to independent Tenant Subscriptions*
* Tenant Software Defined Networks - *Includes a Single VLAN which is allocated to the role of hosting the encapsulated Tenant Traffic*

[graphviz, scvmm-fully-converged, svg]
----
digraph g {
    S1 -> P1 -> U1 -> N1
    U1 -> N2
    P1 -> V1 -> L1 -> R1 -> H1
    L1 -> R2 -> H2
    L1 -> R3 -> H3
    V1 -> L2 -> R4 -> H4
    L2 -> R5 -> H5
    L2 -> R6 -> H6
    V1 -> L3 -> R7 -> H7

    S1 [label="Logical Switch \n Converged"]
    P1 [label="Port Profile \n Full Converged Profile"]
    U1 [label="Physical Uplinks"]
    N1 [label="10Gb pNIC 1"]
    N2 [label="10Gb pNIC 2"]
    V1 [label="Virtual Interfaces"]
    L1 [label="Logical Network \n Management Networks"]
    L2 [label="Logical Network \n Tenant Networks"]
    L3 [label="Logical Network \n Tenant SDN Network"]
    R1 [label="Logical Network Site \n Cluster"]
    R2 [label="Logical Network Site \n Management"]
    R3 [label="Logical Network Site \n Live Migration"]
    R4 [label="Logical Network Site \n Tenant N"]
    R5 [label="Logical Network Site \n Tenant N+1"]
    R6 [label="Logical Network Site \n Tenant N+2"]
    R7 [label="Logical Network Site \n Control Plane"]
    H1 [label="Host Group \n vLAN 160; IP 172.21.60.0/24"]
    H2 [label="Host Group \n vLAN 110; IP 172.21.10.0/24]"]
    H3 [label="Host Group \n vLAN 161; IP 172.21.61.0/24]"]
    H4 [label="Host Group \n vLAN 1000; IP 192.168.0.0/24"]
    H5 [label="Host Group \n vLAN 1001; IP 192.168.1.0/24"]
    H6 [label="Host Group \n vLAN 1002; IP 192.168.2.0/24"]
    H7 [label="Host Group \n vLAN 162; IP 172.21.62.0/24"]
}
----

[graphviz, scvmm-fully-converged, svg]
----
digraph g {
    RTR -> INET [Label="VLAN 199"]
    MGT -> RTR  [Label="VLAN 110"]
    LM -> RTR [Label="VLAN 161"]
    CLST -> RTR [Label="VLAN 160"]
    DMZS -> RTR [Label="VLAN 180"]
    DMZU -> RTR [Label="VLAN 190"]
    SDN  -> RTR [Label="VLAN 162"]
    TN1  -> RTR [Label="VLAN 1001"]
    TN2  -> RTR [Label="VLAN 1002"]
    TN3  -> RTR [Label="VLAN 1003"]
    TN4  -> RTR [Label="VLAN 1004"]
    TN5  -> RTR [Label="VLAN 1005"]
    TN6  -> RTR [Label="VLAN 1006"]
    TN7  -> RTR [Label="VLAN 1007"]
    TN8  -> RTR [Label="VLAN 1008"]
    TN9  -> RTR [Label="VLAN 1009"]

    RTR [Label="Router"]
    INET [Label="Internet"]
    MGT [Label="Management Network"]
    LM  [Label="Live Migration Network"]
    CLST [Label="Cluster Network"]
    DMZU [Label="DMZ Unsecure"]
    DMZS [Label="DMZ Secure"]
    SDN [Label="SDN Control Plane"]
    TN1 [Label="Tenant 1001"]
    TN2 [Label="Tenant 1002"]
    TN3 [Label="Tenant 1003"]
    TN4 [Label="Tenant 1004"]
    TN5 [Label="Tenant 1005"]
    TN6 [Label="Tenant 1006"]
    TN7 [Label="Tenant 1007"]
    TN8 [Label="Tenant 1008"]
    TN9 [Label="Tenant 1009"]
}
----

==== Defining the network elements

===== Logical Networks

The setting we are interested in, is located by navigating to **Fabric** on the *Wunderbar*, then in the *Navigation Tree* expand the node **Networking**; and from here expand the node **Logical Networks**. As we are running from a clean environment, and have already turned off the setting to *Automatically Create Logical Networks* the main pane will be empty.

.SCVMM Network - Logical Network View
image::images/SCVMM2012R2-NETWORK-0008.png[PKI Post Deployment Configuration]

IMPORTANT: In the situation that you have existing logical networks, we are going to essentially ignore these, with the concept that these were all automatically created by VMM, and as we take control of our environment the objective will be to delete all these Logical networks once all hosts are using the new Networking Model.

Based on the design we have already presented for the plan for our environment, our task will be to create a total of 3 Logical Networks.

* Management
* Tenant
* Tenant SDN

To Add a new Logical Network, Right Click the **Logical Networks** node to present the context menu offering **Create Logical Network**, or simply click the same option on the ribbon; which will then launch the **Create Logical Network Wizard**

* On the **Name** page of the wizard, we will start with providing a **Name** and **Description** for the new Logical Network
* We need to make a choice related to how the Logical Networks are physically connected, There are a set of options presented to us as follows
** **One Connected Network** -
** **VLAN-based independent networks** -
** **Private VLAN (PVLAN) network** -

* We are going to choose the following modes
+
[format="csv", options="header", separator="|"]
|===
Logical Network | Networking Mode
Management|**VLAN-Based independent networks**
Tenant|**VLAN-Based independent networks**
Tenant SDN|**One connected network**, with **Allow new VM networks created on this logical network to use network virtualization**
|===
+
.SCVMM Network - Create Logical Network Wizard, Name Page
image::images/SCVMM2012R2-NETWORK-0009.png[PKI Post Deployment Configuration]

* On the **Network Site** page, we can now proceed to define each of the Networks which will be defined per Logical Network grouping.
** The actual process is covered in the topic **Logical Network Sites**
** Once all the Network sites are defined, click **Next**
+
.SCVMM Network - Create Logical Network Wizard, Network Sites
image::images/SCVMM2012R2-NETWORK-0010.png[PKI Post Deployment Configuration]
+

NOTE: If at any point we need to add, edit or remove a Logical Network Site we can edit the properties of the Logical Network.

Using PowerShell we can accomplish the same results, the following example will establish three new logical networks
```powershell
$mgmtLogicalNetwork = New-SCLogicalNetwork -Name "Management" -LogicalNetworkDefinitionIsolation $true -EnableNetworkVirtualization $false -UseGRE $false -IsPVLAN $false -Description "Management Networks, Including Host Access, Live Migration, Storage and Cluster communications"

$tenantLogicalNetwork = New-SCLogicalNetwork -Name "Tenant" -LogicalNetworkDefinitionIsolation $true -EnableNetworkVirtualization $false -UseGRE $false -IsPVLAN $false -Description "Tenant Networks, VLAN Isolated networks for cloud subscribing tenants"

$tenantSNDLogicalNetwork = New-SCLogicalNetwork -Name "Tenant SDN" -LogicalNetworkDefinitionIsolation $false -EnableNetworkVirtualization $true -UseGRE $true -IsPVLAN $false -Description "Tenant Networks [Software Defined], SDN Isolated networks for clod subscribing tenants"
```

===== Logical Network Sites

Adding the *Logical Network Sites*, to a *Logical Network* can be implemented either during execution of the *Logical Networks Wizard*, or any time after the case by simply opening the *Logical Network* properties.

If you are trying locate the *Logical Network Sites*, it is located by navigating to **Fabric** on the *Wunderbar*, then in the *Navigation Tree* expand the node **Networking**; and from here expand the node **Logical Networks**. Now in the *Main Pane* of the window we can see the defined *Logical Networks*. **Right Click** on the *Logical Network* we wish to edit, and from the context menu, select **Properties**

.SCVMM Network - Logical Network Properties Menu
image::images/SCVMM2012R2-NETWORK-0012.png[PKI Post Deployment Configuration]

* The **Logical Network Properties** dialog will be presented. On the **Name** page, we will notice now that, the choice for Networking Isolation is no longer selectable, and instead we will be simply presented with the operation mode for this logical grouping.
+
.SCVMM Network - Logical Network Properties - Name Page
image::images/SCVMM2012R2-NETWORK-0013.png[PKI Post Deployment Configuration]

* On the **Network Sites** page, we can add as many sites are desired to the **Logical Network** group.
+
.SCVMM Network - Logical Network Properties - Sites Page
image::images/SCVMM2012R2-NETWORK-0015.png[PKI Post Deployment Configuration]

* In the main view pane of the dialog, under the *menu* bar, we start by selecting the option **Add**
** The main pane of the dialog will now be dissected to 3 main areas
+
.SCVMM Network - Logical Network Properties Adding a New Site
image::images/SCVMM2012R2-NETWORK-0016.png[PKI Post Deployment Configuration]

*** To the *Left* of the main dialog view, is the **List of Sites** defined within the logical network
*** In the *Upper Right* is the Host Group hierarchy, to identify which **Host Groups** will support the network site we are adding
*** In the *Lower Right* we define the **VLANs** and **IP Subnets** which are allocated to the site, in addition to defining the **Network Site Name**
** To add complete the exercise of adding a Network Site, the following process flow is advised
*** Start, by setting the **Network Site Name**, for example *Management*
*** Then *click* the button **Insert Row**, which will add a row to the *Associated VLANs and IP subnet* list
**** In the **VLAN** field, add the appropriate VLAN number for the selected site, e.g. 110
**** In the **IP Subnet** field, add the appropriate IP Subnet (CDIR Format), e.g. 172.21.10.0/24
*** Finally, In the **Host Groups that can use this network site**, check off the appropriate groups.
+
.SCVMM Network - Logical Network Site Details
image::images/SCVMM2012R2-NETWORK-0017.png[PKI Post Deployment Configuration]
+

NOTE: Understand that the options here will waterfall; that is if you select a parent node in the tree, all its child nodes will be automatically selected. Depending on your requirements you may need to pay attention to the selection, especially if some network sites must only be presented to select host groups.

*** Repeat the process for each additional **Network Site** you wish to add to the **Logical Network**

===== Logical Network Sites Plan
Based on our Plan, we are going to need to populate our all 3 of our logical networks with a number of sites; The following table is an extrapolation of the data which we used to build the Model, with additional information which is specific to matching the physical networks in the environment

[format="csv", options="header", separator="|"]
|===
Logical Network | Network Site Name | VLANs | Subnet | Host Group
*Management*     | *Management*      | 110 | 172.21.10.0/24  | *All Hosts*
Management       | Cluster           | 160 | 172.21.60.0/24    | All Hosts
Management       | Live Migration    | 161 | 172.21.61.0/24    | All Hosts
*Tenant*        | *Management*      | 110 | 172.21.10.0/24  | *All Hosts*
Tenant          | DMZ Unsecure      | 190 |172.21.90.0/24    | All Hosts
Tenant          | DMZ Secure        | 180|172.21.80.0/24    | All Hosts
Tenant          | Tenant vLan 1001  | 1001|192.168.1.0/24   | All Hosts
Tenant          | Tenant vLan 1001  | 1001|192.168.1.0/24   | All Hosts
Tenant          | Tenant vLan 1002  | 1002|192.168.2.0/24   | All Hosts
Tenant          | Tenant vLan 1003  | 1003|192.168.3.0/24   | All Hosts
Tenant          | Tenant vLan 1004  | 1004|192.168.4.0/24   | All Hosts
Tenant          | Tenant vLan 1005  | 1005|192.168.5.0/24   | All Hosts
Tenant          | Tenant vLan 1006  | 1006|192.168.6.0/24   | All Hosts
Tenant          | Tenant vLan 1007  | 1007|192.168.7.0/24   | All Hosts
Tenant          | Tenant vLan 1008  | 1008|192.168.8.0/24   | All Hosts
Tenant          | Tenant vLan 1009  | 1009|192.168.9.0/24   | All Hosts
Tenant          | Tenant vLan 1010  | 1010|192.168.10.0/24  | All Hosts
Tenant SDN      | Control Plane     | 162|172.21.62.0/24    | All Hosts
|===

As you look over the table, you might notice that VLAN 110 has being defined twice in the table, both in the *Management* Logical Network, and the *Tenant* Logical Network.

This is a configuration which you should be careful not to introduce without due consideration. This configuration enables the possibility of authorizing tenant's the ability to deploy Virtual Machines on the *Management* Network. Of course, This might be the objective, and is actually all too common in Enterprises, caution must applied.

There are two primary issues with this configuration:

* The use of IP Pools on these *Logical Network Sites* would potentially be defined more than once, quickly introducing a situation where we could result in duplicate Addresses on the network.
* Security, enabling a Tenant to deploy VMs on a Management Network is clearly a possibility for security issues.

The recommended resolution to these approach is to consider adding a new Logical Network, for example called *Shared Management*. This Network would host the **Logical Network Site** VLANs and IP Addresses scopes which have the requirement of being presented to both the actual Management layer, and the tenants. Taking this approach will also allow us to essentially remove the risk of IP conflicts, as then allocation of VLANs becomes unique again. To assign this Logical network, we would have to map to both *Tenant* space (Virtual Port Profiles), and *Management* space (Uplink Port Profiles) in the later steps.

Taking this change into account in our plan, the table would look as follows:

[format="csv", options="header", separator="|"]
|===
Logical Network  | Network Site Name | VLAN| Subnet  | Host Group
*Shared Management*| *Management*      | 110|172.21.10.0/24  | *All Hosts*
Management          | Cluster           | 160|172.21.60.0/24    | All Hosts
Management          | Live Migration    | 161|172.21.61.0/24    | All Hosts
Tenant             | DMZ Unsecure      | 190|172.21.90.0/24    | All Hosts
Tenant             | DMZ Secure        | 180|172.21.80.0/24    | All Hosts
Tenant             | Tenant vLan 1001  | 1001|192.168.1.0/24   | All Hosts
Tenant             | Tenant vLan 1001  | 1001|192.168.1.0/24   | All Hosts
Tenant             | Tenant vLan 1002  | 1002|192.168.2.0/24   | All Hosts
Tenant             | Tenant vLan 1003  | 1003:|192.168.3.0/24   | All Hosts
Tenant             | Tenant vLan 1004  | 1004|192.168.4.0/24   | All Hosts
Tenant             | Tenant vLan 1005  | 1005|192.168.5.0/24   | All Hosts
Tenant             | Tenant vLan 1006  | 1006|192.168.6.0/24   | All Hosts
Tenant             | Tenant vLan 1007  | 1007|192.168.7.0/24   | All Hosts
Tenant             | Tenant vLan 1008  | 1008|192.168.8.0/24   | All Hosts
Tenant             | Tenant vLan 1009  | 1009|192.168.9.0/24   | All Hosts
Tenant             | Tenant vLan 1010  | 1010|192.168.10.0/24  | All Hosts
Tenant SDN         | Control Plane     | 162|172.21.62.0/24    | All Hosts
|===

Armed with this information we can now proceed to build out the **Network Sites** in our **Logical Networks** leveraging the procedure we just introduced.

Of you prefer, we can also leverage PowerShell to assist us with this exercise:
```powershell
$logicalNetwork = Get-SCLogicalNetwork -Name "Tenant"
Set-SCLogicalNetwork -Name "Tenant" -Description "Tenant Networks, VLAN Isolated networks for cloud subscribing tenants" -LogicalNetwork $logicalNetwork -RunAsynchronously -EnableNetworkVirtualization $false -UseGRE $false -LogicalNetworkDefinitionIsolation $true

$logicalNetworkDefinition = Get-SCLogicalNetworkDefinition -Name "Management" -LogicalNetwork $logicalNetwork

$allSubnetVlan = @()
$allSubnetVlan += New-SCSubnetVLan -Subnet "172.21.90.0/24" -VLanID 190
Set-SCLogicalNetworkDefinition -LogicalNetworkDefinition $logicalNetworkDefinition -Name "DMZ Unsecure" -SubnetVLan $allSubnetVlan -RunAsynchronously

$allHostGroups = @()
$allHostGroups += Get-SCVMHostGroup -ID "0e3ba228-a059-46be-aa41-2f5cf0f4b96e"
$allSubnetVlan = @()
$allSubnetVlan += New-SCSubnetVLan -Subnet "172.21.80.0/24" -VLanID 180

New-SCLogicalNetworkDefinition -Name "DMZ Secure" -LogicalNetwork $logicalNetwork -VMHostGroup $allHostGroups -SubnetVLan $allSubnetVlan -RunAsynchronously

$allHostGroups = @()
$allHostGroups += Get-SCVMHostGroup -ID "0e3ba228-a059-46be-aa41-2f5cf0f4b96e"
$allSubnetVlan = @()
$allSubnetVlan += New-SCSubnetVLan -Subnet "192.168.1.0/24" -VLanID 1001

New-SCLogicalNetworkDefinition -Name "Tenant vLan 1001" -LogicalNetwork $logicalNetwork -VMHostGroup $allHostGroups -SubnetVLan $allSubnetVlan -RunAsynchronously

$allHostGroups = @()
$allHostGroups += Get-SCVMHostGroup -ID "0e3ba228-a059-46be-aa41-2f5cf0f4b96e"
$allSubnetVlan = @()
$allSubnetVlan += New-SCSubnetVLan -Subnet "192.168.2.0/24" -VLanID 1002

New-SCLogicalNetworkDefinition -Name "Tenant vLan 1002" -LogicalNetwork $logicalNetwork -VMHostGroup $allHostGroups -SubnetVLan $allSubnetVlan -RunAsynchronously


```

===== IP Pool
Now that the **Logical Networks** and its contained **Network Sites** have being defined, we can proceed to have **IP Pools** created in Virtual Machine Manager for each of these networks. IP Pools would in a similar manner as we expect from a regular DHCP service, but with a key advantage that the IP addresses which are allocated from the Pool can be implemented as a Static IP assignment on the target.

To create an **IP Pool**, we will navigating to **Fabric** on the *Wunderbar*, then in the *Navigation Tree* expand the node **Networking**; and from here expand the node **Logical Networks**. Now in the *Main Pane* of the window we can see the defined *Logical Networks*. **Right Click** on the *Logical Network* we wish to add the **IP Pool** to; and from the context menu, select **Create IP Pool**

.SCVMM Network - Logical Network Create IP Pool
image::images/SCVMM2012R2-NETWORK-0018.png[PKI Post Deployment Configuration]

* The **Create Static IP Address Pool Wizard** will be presented.
* On the **Name** page of the wizard, we will start with providing a **Name** and **Description** for the new Logical IP Pool, and from the drop down list select the **Logical Network** Network we are adding the pool to.
+
.SCVMM Network - Create Static IP Pool Name and Logical Network
image::images/SCVMM2012R2-NETWORK-0019.png[PKI Post Deployment Configuration]

* On the **Network Site** page, we will now select the **Network Site** which is hosted on the chosen *Logical Network*, that we wish to add the **IP Pool** to.
** By Default the option **Use an existing network site** will be selected, and we will select the appropriate **Network Site** from the drop down list; which will then update the rest of the dialog to present the **IP Subnet**, **VLAN** and **Host Group** which we defined earlier
+
.SCVMM Network - Logical Network IP Pool Network Site Selection
image::images/SCVMM2012R2-NETWORK-0020.png[PKI Post Deployment Configuration]

** Alternatively, If the **Network Site** has not yet being defined, we can select the option **Create a network site**, which will then update the dialog, so that we can provide the relevant details, as we did in the topic **Logical Network Sites** - including providing the **Network Site** Name, the **VLAN** ID for the site, and scoping the **Host Groups**
+
.SCVMM Network - Logical Network IP Pool Creating new Network Site Option
image::images/SCVMM2012R2-NETWORK-0021.png[PKI Post Deployment Configuration]

* On the **IP Address Range** page, we see the page has two areas of focus
** In the section **IP address range** we will define the **Starting IP Address** and **Ending IP Address** for the pool; The wizard will provide some hints by reminding us the defined *IP Subnet* and *Total addresses* included in the pool.
** In the section **VIPs and reserved IP addresses** we are offered the ability to reserve IP addresses from the pool which should be used for **Load Balancing** in the upper field, and reserved for any general reasons, for example IPs currently allocated to systems in the lower field.
+
.SCVMM Network - Logical Network IP Pool Address Range
image::images/SCVMM2012R2-NETWORK-0022.png[PKI Post Deployment Configuration]

* On the **Gateway** page, we will add the default gateway, but clicking on the **Insert** button to add a new row to the dialog. In the **Gateway Address** field provide the IP address for the gateway, if one is applicable for the network pool we are adding.
+
.SCVMM Network - Logical Network IP Pool Gateway
image::images/SCVMM2012R2-NETWORK-0023.png[PKI Post Deployment Configuration]

* On the **DNS** page, we will add the **DNS Server Address** which are appropriate for this IP Pool. These might be existing DNS Servers in your environment, public name servers, or potentially not provided at all, instead the tenant would have to use root hints, if the network gateway allows that traffic to pass.
** Using similar justifications you may want to define the DNS Suffix for the systems using this pool in the **Connection Specific DNS Suffix** field.
+
.SCVMM Network - Logical Network Name Server's
image::images/SCVMM2012R2-NETWORK-0024.png[PKI Post Deployment Configuration]

* On the **WINS** page, I really hope you just click next, as in todayâ€™s world we should not be using this protocol!!
* Finally, on the Summary page, we can review the choices made, before creating the new pool
+
.SCVMM Network - Logical Network IP Pool Summary
image::images/SCVMM2012R2-NETWORK-0025.png[PKI Post Deployment Configuration]


===== IP Pool Plan
Taking the table we established form the initial plan, we will extrapolate this out a little more, this time adding IP and DNS information for each Network Site. This table will then act as the input we will use to build out all the remaining IP Pools for each of our sites.

[format="csv", options="header", separator="|"]
|===
Name | Description | Logical Network | Starting IP | Ending IP | Reserved LB | Reserved | IP Gateway | DNS Servers | DNS Suffix
Management       | |Shared Management| 172.21.10.1 | 172.21.10.254 | | | 172.21.10.1 | 172.21.10.21 | DigiNerve.net
Cluster          | |Management        | 172.21.60.1 | 172.21.60.254 | | |             |              |
Live Migration   | |Management        | 172.21.61.1 | 172.21.61.254 | | |             |              |
DMZ Unsecure     | |Tenant           | 172.21.90.1 | 172.21.90.254 | | | 172.21.90.1 | 172.21.10.21 | DigiNerve.net
DMZ Secure       | |Tenant           | 172.21.80.1 | 172.21.80.254 | | | 172.21.80.1 | 172.21.10.21 | DigiNerve.net
Tenant vLan 1001 | |Tenant           | 192.168.1.1 | 192.168.1.254 | | |             |              |
Tenant vLan 1002 | |Tenant           | 192.168.2.1 | 192.168.2.254 | | |             |              |
Tenant vLan 1003 | |Tenant           | 192.168.3.1 | 192.168.3.254 | | |             |              |
Tenant vLan 1004 | |Tenant           | 192.168.4.1 | 192.168.4.254 | | |             |              |
Tenant vLan 1005 | |Tenant           | 192.168.5.1 | 192.168.5.254 | | |             |              |
Tenant vLan 1006 | |Tenant           | 192.168.6.1 | 192.168.6.254 | | |             |              |
Tenant vLan 1007 | |Tenant           | 192.168.7.1 | 192.168.7.254 | | |             |              |
Tenant vLan 1008 | |Tenant           | 192.168.8.1 | 192.168.8.254 | | |             |              |
Tenant vLan 1009 | |Tenant           | 192.168.9.1 | 192.168.9.254 | | |             |              |
Tenant vLan 1010 | |Tenant           | 192.168.10.1| 192.168.10.254| | |             |              |
Tenant SDN       | |Control Plane    | 172.21.62.1 | 172.21.62.254 | | |             |              |
|===

Using the steps in this topic, we can take this detail and complete the configuration for our environment.
As an alternative, we can use PowerShell to create the IP Pools

```powershell
$logicalNetwork = Get-SCLogicalNetwork -Name "Tenant"

# DNS servers
$allDnsServer = @()

# DNS suffixes
$allDnsSuffixes = @()

# WINS servers
$allWinsServers = @()

$logicalNetworkDefinition = Get-SCLogicalNetworkDefinition -LogicalNetwork $logicalNetwork -Name "Tenant vLan 1001"

$allGateways = @()
$allGateways += New-SCDefaultGateway -IPAddress "192.168.1.1" -Automatic

New-SCStaticIPAddressPool -Name "Tenant vLan 1001 IP Pool" -LogicalNetworkDefinition $logicalNetworkDefinition -Subnet "192.168.1.0/24" -IPAddressRangeStart "192.168.1.1" -IPAddressRangeEnd "192.168.1.254" -DefaultGateway $allGateways -DNSServer $allDnsServer -DNSSuffix "" -DNSSearchSuffix $allDnsSuffixes -RunAsynchronously -Description "IP Pool for the Tenant vLan"
```

The final solution should now be looking similar the following illustration

.SCVMM Network - Logical Network With IP Pools
image::images/SCVMM2012R2-NETWORK-0026.png[PKI Post Deployment Configuration]


One final point to highlight is that VMM **will NOT** check to see if any of the addresses in the associated IP Pool are currently active on the network, therefore it is potentially possibly to introduce some IP Conflicts. To help build the reserved list with the addresses which are active on the network, we can use a simple PowerShell module to ping sweep an IP range, and build a list of active address, which we can then assign to the IP Pool as reserved.

```powershell
Function Reserve-SCStaticIPAddressPoolActiveAddresses
{
    param (
        [String]$Name
    )

    $reservedAddresses = ""
    1..254 | foreach {
        $Address="172.21.10.$_";
        If (test-connection $Address -quiet -count 1) {
            Write-Host "Address '$Address' is in use"
            $reservedAddresses += $Address + ","
        }
    }

    set-SCStaticIPAddressPool -Name $Name -IPAddressReservedSet $reservedAddresses
}
```

===== Virtual Networks

====== Virtual Network, VLAN Isolated
As we begin to deploy the VM Networks, planning for this process is essentially done already. This is because each of the *Logical Networks* we have implemented has a specific *Mode* associated with them.

Refer back to the Logical Networks topic, and you will recall that in our implementation we selected Two modes, respectively, these were:

* VLAN-Based independent networks
* One connected network

The Virtual Networks we are going to focus on initially are those which are hosted in the *VLAN-Based independent networks* configuration

[format="csv", options="header", separator="|"]
|===
Name | Description | Logical Network | Network Site
DigiNerve Management | Management Network | Shared Management | Management
DigiNerve Live Migration | Live Migration Network | Management | Live Migration
DigiNerve Cluster | Cluster Communications Network | Management | Cluster
Tenant 0001 | VLAN Isolated Network for Tenant 0001 | Tenant | Tenant vLAN 1001
Tenant 0002 | VLAN Isolated Network for Tenant 0002 | Tenant | Tenant vLAN 1002
Tenant 0003 | VLAN Isolated Network for Tenant 0003 | Tenant | Tenant vLAN 1003
Tenant 0004 | VLAN Isolated Network for Tenant 0004 | Tenant | Tenant vLAN 1004
Tenant 0005 | VLAN Isolated Network for Tenant 0005 | Tenant | Tenant vLAN 1005
Tenant 0006 | VLAN Isolated Network for Tenant 0006 | Tenant | Tenant vLAN 1006
Tenant 0007 | VLAN Isolated Network for Tenant 0007 | Tenant | Tenant vLAN 1007
Tenant 0008 | VLAN Isolated Network for Tenant 0008 | Tenant | Tenant vLAN 1008
Tenant 0009 | VLAN Isolated Network for Tenant 0009 | Tenant | Tenant vLAN 1009
|===

From the table, we can cleary see a *1:1* relationship between the *VM Network*, and the *Network Site* of the *Logical Network* in a configuration of *VLAN-Based independent networks*

To create a **Virtual Network**, we will navigating to **VMs and Services** on the *Wunderbar*, then in the *Navigation Tree* expand the node **VM Networks**. Now in the *Main Pane* of the window we can see any currently defined *Virtual Networks*. **Right Click** on the *VM Networks* node and from the context menu, select **Create VM Network**

.SCVMM Network - Virtual Networks View
image::images/SCVMM-NETWORK-0022.png[PKI Post Deployment Configuration]

The **Create VM Network Wizard** will be presented.

* On the **Name** page of the wizard, we will start providing
** A **Name** for example *DigiNerve Management*
** A **Description** for example *Management Network*
** On the **Logical Network**, select the appropriate hosting network, eg *Shared Management*
+
.SCVMM Network - Create VM Network Wizard
image::images/SCVMM-NETWORK-0024.png[PKI Post Deployment Configuration]

* On the **Isolation Options** page
** Select the option **Specify a VLAN**, then
** Chose the appropriate **Network Site** associated to the *Logical Network*
+
.SCVMM Network - Create VM Network Isolation Information
image::images/SCVMM-NETWORK-0025.png[PKI Post Deployment Configuration]

* On the **Summary** page, review the setting, the click **Finish**

As per our Plan, we must establish a number of these Virtual Networks in our deployment, therefore we will repeat procedure for each network.

Alternatively, we can also leverage PowerShell to assist.

```powershell
$logicalNetwork = Get-SCLogicalNetwork -Name "Tenant"
$vmNetwork = New-SCVMNetwork -Name "Tenant 0006" -LogicalNetwork $logicalNetwork -IsolationType "VLANNetwork" -Description "VLAN Isolated Network for Tenant 0006"
Write-Output $vmNetwork

$logicalNetworkDefinition = Get-SCLogicalNetworkDefinition -Name "Tenant vLan 1006" -LogicalNetwork $logicalNetwork
$subnetVLAN = New-SCSubnetVLan -Subnet "192.168.6.0/24" -VLanID 1006
$vmSubnet = New-SCVMSubnet -Name "Tenant 0006_0" -LogicalNetworkDefinition $logicalNetworkDefinition -SubnetVLan $subnetVLAN -VMNetwork $vmNetwork
```
Once complete the VM Network List should be populated, and we should also be able to see any IP Pools which were associated to the Network Site

.SCVMM Network - VM Networks View Post Configuration
image::images/SCVMM-NETWORK-0026.png[PKI Post Deployment Configuration]

If you have a lot of VM Networks which you might wish to map, the following function will enumerate the Logical Networks which have being established, and are defined as Isolated VLANs; skipping the VMNetworks already in place, and just adding the ones missing.

```powershell

$logicalSites = Get-SCLogicalNetworkDefinition | select @{ expression={$_.name}; label="LogicalNetworkDefinition"}, LogicalNetwork
$VMNetworks = Get-SCVMSubnet | select VMNetwork, LogicalNetworkDefinition
$MissingVMNetworks = Compare-Object -ReferenceObject $LogicalSites -DifferenceObject $VMNetworks -Property LogicalNetworkDefinition, LogicalNetworkDefinition -PassThru
foreach ($VMNetwork in $MissingVMNetworks) {
    If ($vmNetwork.SideIndicator -eq "<=") {
        #Need to create the VM Network for this LogicalNetowrkDefination (Site)
        $ourLogicalNetwork = $vmNetwork.LogicalNetwork
        $ourLogicalNetworkDefination = $vmNetwork.LogicalNetworkDefination

        $logicalNetwork = Get-SCLogicalNetwork -Name $ourLogicalNetwork
        if ($logicalNetwork.IsLogicalNetworkDefinitionIsolated) {
            Write-Output "Creating VM Network for Site '$ourLogicalNetworkDefination' in Logical Network '$ourLogicalNetwork'"
            $vmNetwork = New-SCVMNetwork -Name $ourLogicalNetworkDefination -LogicalNetwork $logicalNetwork -IsolationType "VLANNetwork"
            Write-Output $vmNetwork
            $logicalNetworkDefinition = Get-SCLogicalNetworkDefinition -Name $ourLogicalNetworkDefination -LogicalNetwork $logicalNetwork

            $ourSubnet = $logicalNetworkDefinition.SubnetVlans.Subnet
            $ourVLAN =  $logicalNetworkDefinition.SubnetVlans.VLanID
            Write-Output "Assigning Subnet '$ourSubnet' and VLAN '$outVLAN' to the Site"
            $subnetVLAN = New-SCSubnetVLan -Subnet $ourSubnet -VLanID $ourVLan
            $vmSubnet = New-SCVMSubnet -Name "$ourLogicalNetworkDefination Subnet" -LogicalNetworkDefinition $logicalNetworkDefinition -SubnetVLan $subnetVLAN -VMNetwork $vmNetwork
        } else {
            Write-Output "Skipping VM Network for Site '$ourLogicalNetworkDefination' in Logical Network '$ourLogicalNetwork' as it is not in Isolated Mode"
        }
    }

}

```

===== Port Profiles

To create an **Uplink Port Profile**, we will navigating to **Fabric** on the *Wunderbar*, then in the *Navigation Tree* expand the node **Networking**; and from here expand the node **Port Profiles**.

.SCVMM Network - Default Port Profiles View
image::images/SCVMM2012R2-NETWORK-0027.png[PKI Post Deployment Configuration]

====== Uplink Port Profile
We will proceed with creating a new Uplink Port Profile for our hosts to connect to their fabric.

To create an **Uplink Port Profile**, **Right Click** on the *Port Profiles* node and from the context menu, select **Create Hyper-V Port Profile**

.SCVMM Network - Create Hyper-V Port Profile
image::images/SCVMM-Network-0000.png[PKI Post Deployment Configuration]

The **Create Hyper-V Port Profile Wizard** will be presented.

* On the **General** page of the wizard, we will start providing
** A **Name** for example *Converged 1Gb Uplinks*
** A **Description** for example *Port Profile for a fully converged 1Gb Host, which will support Management and Tenant Traffic*
+
.SCVMM Network - Create Hyper-V Port Profile General Page
image::images/SCVMM-Network-0001.png[PKI Post Deployment Configuration]

** The **Type of Hyper-V port profile** selection, choose the option **Uplink Port Profile** Network we are adding the pool to.
*** For the **Load Balancing algorithm**, we can select the **Host Default**
*** For the **Teaming Mode**, we can select **Switch Independent**
** Click **Next**
* On the **Network Configuration** page, we will now select the **Network Site** which we will traverse over this physical network connection.
** In the Scenario of a *Fully Converged* environment, then the choice is going to be pretty simple, we will generally be selecting all the *Network Sites* to traverse this connection.
** In a partially converged scenario, we would of course be selecting only the appropriate networks sits to traverse the particular uplink, for example it would be common to have one uplink profile dedicated to converging Management related workloads. The only exception maybe if we were going to use an additional *Uplink Port Profile* to is hosted on the chosen *Logical Network*, that we wish to add the **IP Pool** to.
+
.SCVMM Network - Create Hyper-V Port Profile Network Configuration Page
image::images/SCVMM-Network-0007.png[PKI Post Deployment Configuration]

** By Default the option **Enable Hyper-V Network Virtualization** will be cleared. You should **Enable** this option when you choose to assign the *SDN Control Plane* as one of your *Network Sites* for this *Uplink Port Profile*. On Hyper 2012 hosts, this will then instruct VMM to load the HNV Filter to the Virtual Switch; this has NO EFFECT on Hyper-V 2012 R2 and later versions as this filter is installed and enabled by default!
* Finally, on the **Summary** page, we will get the opportunity to review the choices we have made, before creating the new Port Profile
+
.SCVMM Network - Create Hyper-V Port Profile Summary Page
image::images/SCVMM-Network-0008.png[PKI Post Deployment Configuration]


If you wish, we can of course accomplish the same using PowerShell

```powershell
# Create the List of Network Sites we will add to the profile
$definition = @()
$definition += Get-SCLogicalNetworkDefinition -Name "Cluster"
$definition += Get-SCLogicalNetworkDefinition -Name "DMZ Unsecure"
$definition += Get-SCLogicalNetworkDefinition -Name "Management"
$definition += Get-SCLogicalNetworkDefinition -Name "Live Migration"
$definition += Get-SCLogicalNetworkDefinition -Name "Control Plane"
$definition += Get-SCLogicalNetworkDefinition -Name "DMZ Secure"
$definition += Get-SCLogicalNetworkDefinition -Name "Tenant vLan 1001"
$definition += Get-SCLogicalNetworkDefinition -Name "Tenant vLan 1002"
$definition += Get-SCLogicalNetworkDefinition -Name "Tenant vLan 1003"
$definition += Get-SCLogicalNetworkDefinition -Name "Tenant vLan 1004"
$definition += Get-SCLogicalNetworkDefinition -Name "Tenant vLan 1005"
$definition += Get-SCLogicalNetworkDefinition -Name "Tenant vLan 1006"
$definition += Get-SCLogicalNetworkDefinition -Name "Tenant vLan 1007"
$definition += Get-SCLogicalNetworkDefinition -Name "Tenant vLan 1008"
$definition += Get-SCLogicalNetworkDefinition -Name "Tenant vLan 1009"

# Create the new Profile, and associate the array of sites we created
New-SCNativeUplinkPortProfile -Name "Converged 1Gb Uplinks" -Description "Port Profile for a fully converged 1Gb Host, which will support Managment and Tenant Traffic" -LogicalNetworkDefinition $definition -EnableNetworkVirtualization $true -LBFOLoadBalancingAlgorithm "HostDefault" -LBFOTeamMode "SwitchIndependent" -RunAsynchronously
```

===== Logical Switch
We will proceed with creating our Logical Switch

To create a **Logical Switch**, we will navigating to **Fabric** on the *Wunderbar*, then in the *Navigation Tree* expand the node **Networking**; and from here expand the node **Logical Switches**. Now in the *Main Pane* of the window we are expecting that there are currently no switches defined. **Right Click** on the *Logical Switches* node and from the context menu, select **Create Logical Switch**

.SCVMM Network - Create Hyper-V Logical Switch
image::images/SCVMM-Network-0010.png[PKI Post Deployment Configuration]

* On the **Getting Started** page, read the overview, and one ready click on **Next**
+
.SCVMM Network - Create Hyper-V Logical Switch Getting Started Page
image::images/SCVMM-Network-0011.png[PKI Post Deployment Configuration]

* On the **General** page,
** A **Name** for example *Converged Switch*
** A **Description** for example *Fully Converged Logical Switch for our hosts*
+
.SCVMM Network - Create Hyper-V Logical Switch General Page
image::images/SCVMM-Network-0012.png[PKI Post Deployment Configuration]

** Only *Enable* the option **Enable Single Root I/O Virtualization (SR-IOV)** if your Network Interfaces support this setting, and you understand the Pro's and Con's associated.
+
NOTE: Setting the SR-IOV option is only available at the initial creation of the Logical Switch, if you later need to change your selection, you will need to establish a new Logical Switch, and potentially remove the existing Logical Switch

** Click **Next** to proceed
* On the **Extensions** Page, we will not make any changes, as the default Extensible Switch settings are perfect for now
+
.SCVMM Network - Create Hyper-V Logical Switch Extensions Page
image::images/SCVMM-Network-0013.png[PKI Post Deployment Configuration]

* On the **Uplink** Page
** In the **Uplink Mode** drop down list we will set the uplink as appropriate for our hosts, Normally this will be set to **Team**
+
.SCVMM Network - Create Hyper-V Logical Switch Uplink Page
image::images/SCVMM-Network-0014.png[PKI Post Deployment Configuration]

** For the **Uplink Port Profiles**, we will need to add the *Uplink Port Profile* we defined earlier. Click on the **Add...** button
*** In the **Add Uplink Port Profile** dialog, Select the appropriate *Port Profile* from the drop down list, then click **OK**
+
.SCVMM Network - Add Upload Port Profile
image::images/SCVMM-Network-0015.png[PKI Post Deployment Configuration]

** If additional *Uplink Port Profiles* were defined and appropriate for this *Logical Switch* we can repeat the *Add* process; once ready then click **Next** to move forward
* On the **Virtual Port** Page
** We will need to add the *Virtual Port Profile's* which are appropriate for the traffic we expect to traverse this switch and potentially traverse the Uplink Interfaces.
+
.SCVMM Network - Create Hyper-V Logical Switch Virtual Port Page
image::images/SCVMM-Network-0016.png[PKI Post Deployment Configuration]

** As the *Logical Switch* we are defining is for a *Full Converged* configuration, we will need to add Virtual Profiles for both Host Management and Virtual Machine workloads.
+
[format="csv", options="header", separator="|"]
|===
Port Classification     | Port Profile
Host Management         | Host Management
Host Cluster Workload   | Cluster
Live migration Workload | Live Migration
Low Bandwidth           | Low Bandwidth Adapter
Medium Bandwidth        | Medium Bandwidth Adapter
High Bandwidth          | High Bandwidth Adapter
|===

** To add a **Virtual Port Classification**, Click on the **Add...** button
*** In the **Add Virtual Port** dialog, for the **Port Classification** select the **Browse Button**
+
.SCVMM Network - Add Virtual Port Dialog
image::images/SCVMM-Network-0017.png[PKI Post Deployment Configuration]

**** In the **Select a Port Profile Classification** dialog, from the list of defined classifications, select one which is appropriate to add to the switch. For example, the initial classification on the list appropriate for this switch is that of *Host Management*, then click **OK**
+
.SCVMM Network - Select a Port Profile Classification
image::images/SCVMM-Network-0018.png[PKI Post Deployment Configuration]

*** **Enable** the option **Include a virtual network adapter port profile in this virtual port**
*** From the **Native virtual network adapter port profile** drop down list, select the matching profile for the selected classification, for example *Host Management*
*** Click **Ok** to save this Port Classification
** Repeat this step for each of the classifications, that is appropriate for your *Logical Switch*. Once complete, click **Next**
+
.SCVMM Network - Create Hyper-V Logical Virtual Port Page
image::images/SCVMM-Network-0019.png[PKI Post Deployment Configuration]

** On the **Summary** page, review the settings we have defined, and once you are ready, click on **Finish** to create the *Logical Switch*
+
.SCVMM Network - Create Hyper-V Logical Switch Summary Page
image::images/SCVMM-Network-0020.png[PKI Post Deployment Configuration]

To accomplish the same switch using PowerShell, we can execute the following

```powershell
#First Define the Logical Swtich Extensions we will enable
$virtualSwitchExtensions = @()
$virtualSwitchExtensions += Get-SCVirtualSwitchExtension -Name "Microsoft Windows Filtering Platform"

# Next, we will create the Logical Swtich, and store a reference to it
$logicalSwitch = New-SCLogicalSwitch -Name "Converged Switch" -Description "Fully Converged Logical Switch for our hosts" -EnableSriov $false -SwitchUplinkMode "Team" -VirtualSwitchExtensions $virtualSwitchExtensions

# With the Logical Swtich Created, we will attach Uplink Port Profiles
$nativeProfile = Get-SCNativeUplinkPortProfile -Name "Converged 1Gb Uplinks"
New-SCUplinkPortProfileSet -Name "Converged 1Gb Uplinks" -LogicalSwitch $logicalSwitch -RunAsynchronously -NativeUplinkPortProfile $nativeProfile

# In a Similar process, we will define the Virtual Port Classifications,
# Port Profiles, and then attach them to the logical Switch

# Host Management
$portClassification = Get-SCPortClassification -Name "Host management"
$nativeProfile = Get-SCVirtualNetworkAdapterNativePortProfile -Name "Host management"
New-SCVirtualNetworkAdapterPortProfileSet -Name "Host management" -PortClassification $portClassification -LogicalSwitch $logicalSwitch -RunAsynchronously -VirtualNetworkAdapterNativePortProfile $nativeProfile


$portClassification = Get-SCPortClassification -Name "Host Cluster Workload"
$nativeProfile = Get-SCVirtualNetworkAdapterNativePortProfile -Name "Cluster"
New-SCVirtualNetworkAdapterPortProfileSet -Name "Host Cluster Workload" -PortClassification $portClassification -LogicalSwitch $logicalSwitch -RunAsynchronously -VirtualNetworkAdapterNativePortProfile $nativeProfile

$portClassification = Get-SCPortClassification -Name "Live migration  workload"
$nativeProfile = Get-SCVirtualNetworkAdapterNativePortProfile -Name "Live migration"
New-SCVirtualNetworkAdapterPortProfileSet -Name "Live migration  workload" -PortClassification $portClassification -LogicalSwitch $logicalSwitch -RunAsynchronously -VirtualNetworkAdapterNativePortProfile $nativeProfile

$portClassification = Get-SCPortClassification -Name "Low bandwidth"
$nativeProfile = Get-SCVirtualNetworkAdapterNativePortProfile -Name "Low Bandwidth Adapter"
New-SCVirtualNetworkAdapterPortProfileSet -Name "Low bandwidth" -PortClassification $portClassification -LogicalSwitch $logicalSwitch -RunAsynchronously -VirtualNetworkAdapterNativePortProfile $nativeProfile

$portClassification = Get-SCPortClassification -Name "Medium bandwidth"
$nativeProfile = Get-SCVirtualNetworkAdapterNativePortProfile -Name "Medium Bandwidth Adapter"
New-SCVirtualNetworkAdapterPortProfileSet -Name "Medium bandwidth" -PortClassification $portClassification -LogicalSwitch $logicalSwitch -RunAsynchronously -VirtualNetworkAdapterNativePortProfile $nativeProfile

$portClassification = Get-SCPortClassification -Name "High bandwidth"
$nativeProfile = Get-SCVirtualNetworkAdapterNativePortProfile -Name "High Bandwidth Adapter"
New-SCVirtualNetworkAdapterPortProfileSet -Name "High bandwidth" -PortClassification $portClassification -LogicalSwitch $logicalSwitch -RunAsynchronously -VirtualNetworkAdapterNativePortProfile $nativeProfile
```

Once completed, we should see our new Logical Switch created.
![SCVMM 2012R2 - Fabric Rename Host Group](media\scvmm-configuration-networks\SCVMM-Network-0021.png)

==== Deploying a Logical Switch for Management

Establishing our first Logical switch on a host can be a little challenging, however the only point we need to be concerned with, is that we MUST always sustain a working Layer 3 communication path between the VMM server and the Agent on the Host.

If our host has no VMM agent, we would first want to deploy the agent to the host; refer to the topic **Adding Hosts** in the Article **Host Groups**

===== Physical Interfaces
We are going begin with a focus on the Management Team. In a *Full Converged* implementation this will the only logical switch we need to focus on.

We are also going to assume this initial team of physical interfaces (pNICs) will comprise of at least 2 member adapters. Additionally, to enable and sustain VMM communications with the host, we can assume that at least one of these adapters is configured with a working IP address in the Management Network Site.

The IP address assigned can be in one of two scenarios

* The IP address is assigned temporarily, to support getting the Host configured trough VMM
* The IP address assigned is the final address for the host, contained in the Management Network Site, and we would require VMM to retain this address.

To assist in the deployment, we are going to use an approach of imagining a Label on the two physical network interfaces

* Temporarily Assigned Management IP - Consider the interface with the IP address is *Member Interface B*
* Parentally Assigned Management IP - Consider the interface with the IP address is *Member Interface A*

If, both Interfaces have IPs assigned, the second address is essentially irrelevant, and will be dropped from the host once the configuration is complete.

Working with temporarily assigned Management IPs is normally a little less risky, and results in a higher success rate for deployments. Our primary objective is to attempt to ensure that the VMM server and this host can continue to communicate while the new Logical Switch for Management is being established.

===== Switch Deployment with Temporary Assigned Management IP
In this scenario, the IP address on this *Member Interface B* is temporary and only required for this purpose of sustaining this initial communications. Therefore as we build out *Member Interface A* to participate in the team we will also apply the new Permanent IPv4 address which will be leveraged as the management address for this host. This address will be allocated from the IP Pool we have created for the management VM Network.

===== Physical Team - First Interface
Navigate to **Fabric** on the *Wunderbar*, then in the *Navigation Tree* expand the node **Networking**; and from here expand the node **Logical Networks**. Now in the *Main Pane* of the window we can see the defined *Logical Networks*. In the Ribbon, change the view pivot to  **Hosts**. The main pane will now present the hosts in the fabric, alone with their physical and virtual interfaces (only if VMM created these), and the virtual switch.

.SCVMM Network - Logical Networks Host View
image::images/SCVMM-Network-0027.png[PKI Post Deployment Configuration]

* Open the **Properties** of the host, and select the *Virtual Switches* tab
+
.SCVMM Network - Distributed Switch Settings
image::images/SCVMM-Network-0028.png[PKI Post Deployment Configuration]

.Existing Distributed Switch
****
If a Distributed Switch is present, we may need to ensure that the switch is not using all of our Physical NIC, in its team; as this would prevent us from establishing any new team due to having no available network interfaces. We will need to ensure that we have at least one pNIC available before we can establish the new switch.

Virtual Machine Manager will offer the option to change the configuration of the interfaces, for example, we could select the **External** option, and change the **Network Adapter** from the teamed interface, which is by default called the *Microsoft Network Adapter Multiplex Driver* to one of the physical interfaces, for example *Broadcom NetXtreme Gigabit Ethernet* as per my sample host

.SCVMM Network - Select a Physical Interface connected to the Distributed Switch
image::images/SCVMM-Network-0029.png[PKI Post Deployment Configuration]

* Clicking **OK** in the Dialog will then invoke VMM to issue us a warning, that this change may leave the node inaccessible for some moments while the job executes
+
.SCVMM Network - Confirm It is Ok to Remove the Interface
image::images/SCVMM-Network-0030.png[PKI Post Deployment Configuration]


In reality, this job will fail 75% of the time, as VMM will break the Virtual Switch configuration, and while doing so, prevent the Layer 3 communication with the the Agent; leaving the host isolated, and failing to complete the host configuration. In most cases the host will be broken as it may have stopped mid configuration, leaving the node requiring human intervention


To release at least one of the Physical Interfaces on the target host, make the change directly on the node, to ensure communications is retained.

* Connect to the target host, and using the **Server Manager**, switch to the **Local Server** tab. **NIC Teaming** will be *Enabled*, click on the link to launch the **NIC Teaming** interface.
+
.SERVER MANAGER - NIC Teaming
image::images/SCVMM-Network-0031.png[PKI Post Deployment Configuration]

* In the Lower Left, locate the **Teams** list, and double click the team *Distributed Switch* to open the **Team Properties** dialog.
* Clear the **Check** for one of the interfaces in the **Member Adapters** list, to have it released from the Team, then click **OK** to apply the change.
+
.SERVER MANAGER - NIC Teaming Team Members
image::images/SCVMM-Network-0032.png[PKI Post Deployment Configuration]

* We should then see the UI update, to confirm in the Lower Right of the **NIC Teaming** dialog, under the heading **Adapters and Interfaces** locate that our Physical Interface is now listed in the subgroup *Available to be added to a team*. Make a mental note of the Label on the interface we have just released.
+
.SERVER MANAGER - NIC Teaming Interface Removed
image::images/SCVMM-Network-0033.png[PKI Post Deployment Configuration]


Refresh the Host in VMM to have it updated with the current configuration of the host. Once complete, reopen the Host Properties, and navigate back to the *Virtual Switch* Tab

****

* Click the option located at the top for the Dialog **New Virtual Switch**, and on the Drop Down options, Select the choice **New Logical Switch**
+
.SCVMM Network - Virtual Switch Properties - New Logical Switch
image::images/SCVMM-Network-0034.png[PKI Post Deployment Configuration]

** In the main pane of the dialog, set the **Logical Switch** drop down to match the switch you are about to deploy. for example I have just the *Converged Switch*; but if this is your initial switch it might be called *Management Switch*
** In **Physical Adapters** list, we should see that 1 Physical Interface is added for us.
*** Check that the name of the **Adapter** in the drop down under the column **Adapter** matches the Interface we mentally made a note to use.
*** In the **Uplink Port Profile** column, set the associated profile you plan to use with this physical connection, if only one was defined on the Logical Switch, we do not need to make any additional changes.
+
.SCVMM Network - Virtual Switch Properties - Logical Switch Details
image::images/SCVMM-Network-0035.png[PKI Post Deployment Configuration]

* Next, we will consider the **Virtual Network Interface** to add to the host.
* **Do Not Click OK yet!**

===== Virtual Interfaces

Next, while still in the same dialog, we will move our focus to the Virtual Interfaces which will be created on the Logical Switch. For the *Management* or *Full Converged* Switch we will be addressing at least 3 *Virtual Interfaces* for this work

1. Management Virtual Network (To assume the management traffic from VMM to the Host)
2. Cluster Virtual Network (To Assume the cluster related traffic between hosts)
3. Live Migration Network (to Assume the traffic which is generated while moving VMs between hosts)

Back in the Interface

* Click the Name of the Logical Switch, for example **Converged Switch** or **Management Switch**
+
.SCVMM Network - Virtual Switch Properties - New Virtual Network Adapter
image::images/SCVMM-Network-0036.png[PKI Post Deployment Configuration]

* Click the Menu option located at the top for the Dialog **New Virtual Network Adapter**
** A new Virtual Adapter will be added to the UI under the *Logical Switch*, This we will use for **Management** Traffic
** Set the **Name** for this new Interface to **Management**
** Uncheck the option **This virtual adapter inherits settings from the physical management adapter** as the *Member Interface A* has no IP address assigned we would wish to claim
** Set the **VM Network** to **DigiNerve Management**, by clicking on the **Browse...** button and selecting the network from the list presented in the **Select a VM Network** dialog
** Set the **VM Subnet** to **DigiNerve Management**
** Set the **Port Profile** drop down to **Host Management Workload**
** Set the **IP Address Configuration** to **Static**
*** In the drop down for **IPv4 pools** select the **Management Network IP Pool**
** Set the **MAC Address** to **Static**
*** In the field, we can leave the default value of *00:00:00:00:00:00*, which indicates to VMM that this needs to be replaced with an address from the *MAC Address Pool*
* For the Second Virtual Interface we will repeat the addition, by again Clicking on the Name of the Logical Switch, for example **Converged Switch** or **Management Switch**
+
.SCVMM Network - Virtual Switch Properties - Virtual Network Interface
image::images/SCVMM-Network-0037.png[PKI Post Deployment Configuration]

* From the Menu at the top for the dialog, choose **New Virtual Network Adapter**
** Our second new Virtual Adapter will be added to the UI under the *Logical Switch*, and *Management* virtual interface, This we will use for **Cluster** Traffic
*** Set the **Name** for this new Interface to **Cluster**
*** The option **This virtual adapter inherits settings from the physical management adapter** will be unchecked, leave it this way
*** Set the **VM Network** to **DigiNerve Cluster**, by clicking on the **Browse...** button and selecting the network from the list presented in the **Select a VM Network** dialog
*** Set the **VM Subnet** to **DigiNerve Cluster**
*** Set the **Port Profile** drop down to **Host Cluster Workload**
*** Set the **IP Address Configuration** to **Static**
**** In the drop down for **IPv4 pools** select the **Cluster Network IP Pool**
*** Set the **MAC Address** to **Static**
**** In the field, we can leave the default value of *00:00:00:00:00:00*, which indicates to VMM that this needs to be replaced with an address from the *MAC Address Pool*
* For the Third Virtual Interface we will repeat the addition, by again Clicking on the Name of the Logical Switch, for example **Converged Switch** or **Management Switch**
+
.SCVMM Network - Virtual Switch Properties - Clustering Virtual Network Interface
image::images/SCVMM-Network-0038.png[PKI Post Deployment Configuration]

* From the Menu at the top for the dialog, choose **New Virtual Network Adapter**
** Our second new Virtual Adapter will be added to the UI under the *Logical Switch*, and *Management* and *Cluster* virtual interface, This we will use for **Live Migration** Traffic
*** Set the **Name** for this new Interface to **Live Migration**
*** The option **This virtual adapter inherits settings from the physical management adapter** will be unchecked, leave it this way
*** Set the **VM Network** to **DigiNerve Live Migration**, by clicking on the **Browse...** button and selecting the network from the list presented in the **Select a VM Network** dialog
*** Set the **VM Subnet** to **DigiNerve Live Migration**
*** Set the **Port Profile** drop down to **Live migration Workload**
*** Set the **IP Address Configuration** to **Static**
**** In the drop down for **IPv4 pools** select the **Live Migration Network IP Pool**
*** Set the **MAC Address** to **Static**
**** In the field, we can leave the default value of *00:00:00:00:00:00*, which indicates to VMM that this needs to be replaced with an address from the *MAC Address Pool*
* Now we have finished the mapping of the logical switch to our host, using the free physical interface as the uplink to the switch, and adding a number of virtual interfaces which will be used by the Management OS for its operations.
+
.SCVMM Network - Virtual Switch Properties - Logical Switch Details
image::images/SCVMM-Network-0039.png[PKI Post Deployment Configuration]

* Click **OK** To start the Job
* VMM will offer a warning, asking if we are sure to continue. As we are using just a single management interface for the team, we are pretty safe, as not to lose communications with the host.

If you connect to the host, you can watch *Network Connections* change as the new settings are applied. The process will take a couple of minutes to complete.

NOTE: As VMM is using a Pool to assign IP addresses to these new Network Interfaces, be careful to ensure that you do not introduce an IP Conflict, as VMM does not apply any checks to the fabric before assigning the address! A helper function to maintain a reserved list of IP addresses is covered in the topic for IP Pools

===== Verify the Deployment
We expect to see the 3 vNIC's created, and assigned static IP addresses from the pool. From the VMM server try ping the new Management IP address, to ensure that Layer 3 is up and working. If you are unable to ping the assigned management address, you must resolve this issue before progressing.

.SCVMM Network - Virtual Interface Compliant
image::images/SCVMM-Network-0047.png[PKI Post Deployment Configuration]

If we investigate the properties of any of the *Virtual Network Interfaces* we just implemented, we should see that an IP address was allocated from the pool, and assigned to the interface.

.SCVMM Network - Virtual Interface Static IP
image::images/SCVMM-Network-0048.png[PKI Post Deployment Configuration]


We should also be able to connect to the host, and verify that this address was statically assigned, using a simple  *ipconfig* command, and while we are there validate communications with a *ping* back to the VMM Server, and of course from the VMM Server to this new management IP Address

.Windows Server - In Logical Switch Configuration
image::images/SCVMM-Network-0049.png[PKI Post Deployment Configuration]

Checking the Hyper-V service, we should now also see that the original Distributed Switch is still deployed, but more importantly, we should now see our new *Logical Switch* which we called *Converged Switch* in VMM.  Additionally, you will notice that all the configuration options for the Switch are greyed out, preventing any changes, but also not honestly reflecting the configuration of the switch in this dialog - Donâ€™t Panic, this is exactly what is expected, as we are now looking at a Logical Switch; all settings are managed centrally from the VMM Console.


===== Physical Team - Second and Subsequent Interface's
With the new Management IP address verified as live, and suitable from communication, we can now proceed to move the *Member Interface B* into the team. As we execute this process, we deallocate the IP address which has being assigned to this interface, leaving the host to conduct its management communications on the Virtual interface of our team only.

Navigate to **Fabric** on the *Wunderbar*, then in the *Navigation Tree* expand the node **Networking**; and from here expand the node **Logical Networks**. Now in the *Main Pane* of the window we can see the defined *Logical Networks*. In the Ribbon, change the view pivot to  **Hosts**. The main pane will now present the hosts in the fabric, alone with their physical and virtual interfaces (only if VMM created these), and the virtual switch.

* Open the **Properties** of the host, and select the *Virtual Switches* tab

.Existing Distributed Switch
****

If a Distributed Switch is present, we are most likely done with it now, and need to release any interfaces still attached. Therefore now is an appropriate time to delete this switch.

* Click on the name of the Distributed Switch, for example **Distributed Switch**

* On the *Ribbon* click the option **Delete**
+
.SCVMM Network - Existing Distributed Switch to Remove
image::images/SCVMM-Network-0056.png[PKI Post Deployment Configuration]

* The interface will update, now showing just our remaining *Logical Switch*

****

* Click on the Name of the Logical Switch, for example **Converged Switch** or **Management Switch**
+
.SCVMM Network - Converged Logical Switch
image::images/SCVMM-Network-0053.png[PKI Post Deployment Configuration]

* In **Physical Adapters** list, we should see our initial Physical Interface. We will now click on the **Add** button to append a second interface to the list.
* Change the **Adapter** in the drop down under the column **Adapter** to match the name of the second interface which we will be adding to this team, in my example this will be *Broadcom 1Gb Bottom*.
+
.SCVMM Network - Add additional Physical Interfaces to the Logical Switch
image::images/SCVMM-Network-0054.png[PKI Post Deployment Configuration]

* In the **Uplink Port Profile** column, set the associated profile you plan to use with this physical connection, if only one was defined on the Logical Switch, we do not need to make any additional changes.
+
.SCVMM Network - Physical Adapters added and port profiles selected
image::images/SCVMM-Network-0055.png[PKI Post Deployment Configuration]

+
NOTE: If there are additional Physical Interfaces, we can repeat these steps for each interface to be added to the team

* Click **Ok** to commit the changes
* VMM will pop up a warning, that this change could break communications, accept this by clicking **OK**
+
.SCVMM Network - VMM Warning that the change may break connections
image::images/SCVMM-Network-0057.png[PKI Post Deployment Configuration]


This new job should complete; it is possible that you may lose connections with the host if you are logged in with RDP, as the DNS for the node might not have updated with the new addresses yet. In these cases we can use the command line option to flush are reload DNS

```cmd
ipconfig /flushDNS
```

We should again check that Layer 3 communications are still active to the VMM management server, simply pining VMM from the host, and if that fails, verify that you can at least ping another management interface in the same network, or its gateway. Layer 3 IP communications must be working correctly before progressing.

.SCVMM Network - Final Configured Host View
image::images/SCVMM-Network-0058.png[PKI Post Deployment Configuration]


==== Establish a Logical Switch for Tenant Workloads

With the difficult work of getting the Management Logical switch online and working, we can now proceed to establish any subsequent Logical switches on the hosts. These additional switches are far less concerning to deploy, because we no longer need to be concerned about possibly breaking the management communications between the host and the VMM server

===== Physical Interfaces
Focusing on the Tenant Traffic Team, we can again assume the team will have at least 2 adapters as members; neither of these will have IP addresses assigned once we are complete, as the traffic to flow over this team will be sourced from within the VMs, or via an extension on the Hyper-V Switch; for example Software Defined Networking.

====== Physical Team - All Interfaces
While using the *Fabric View*, with the *Hosts Pivot* selected, we start updating network options as follows:

* Open the **Properties** of a newly added host, and select the *Virtual Switches* tab
* Click the option located at the top for the Dialog **New Virtual Switch**
** In the Drop Down Select the option **New Logical Switch**
** Assign a name for the new Logical Switch, e.g. *Tenant Switch*
* In Physical Adapters pane, we should see that 1 Physical Interface is listed.
** Chose from the Drop Down *Adapter* list, the name which represents our *Tenant pNIC Interface A*
* Still focused to the Physical Adapters pane, click the *Add* button on the right of the dialog.
* We should see that 2 Physical Interface are listed.
** From the Drop Down, select the name of the newly added second interface
** Chose from the Drop Down *Adapter* list, the name which represents our *Tenant pNIC Interface B*
* To have VMM add the selected Physical Interface to the Team, simply Click **OK** to start the job
* Click **OK** To start the Job

===== Virtual Interfaces - Software Defined Networking
If we are going to use Software Defined Networking on this host, we can add the *Provider Network* as a Virtual Interface to this Logical Switch. This network will be used to carry the encapsulated traffic between the VM's which are running on this host, and other hosts, or a gateway.

To add the provider network, we will add the Virtual Interface to the Logical Switch

* Open the **Properties** of the host, and select the *Virtual Switches* tab
* Click the Name of the Logical Switch, e.g. *Tenant Switch*
* Click the option located at the top for the Dialog **New Virtual Network Adapter**
** A new Virtual Adapter will be added to the UI under the Logical Switch
*** Name the Interface **Provider**
*** Set the Name of the Network
*** Uncheck the option **This virtual adapter inherits settings from the physical management adapter**
*** Set the VM Network to **Provider Network**
*** Set the VM Subnet to **Provider Network**
*** Set the Port Profile **Provider Workload**
*** Set the IP Pool for Static
* Click **OK** To start the Job

The assigned Provider address will not be pingable using normal ping command, if you test from the host you just deployed to; therefore you will need to use the special provider ping tool

```cmd
pping PROVIDER_IP_ADDRESS
```

===== Subsequent Hosts
With the concept for the process of deploying the Logical Network now completed on at least one node, we can simply take this same process and repeat it for your subsequent hosts.
